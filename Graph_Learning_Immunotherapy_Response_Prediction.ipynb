{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6954a002-d7af-4407-bc7c-92ebfae0af72",
   "metadata": {},
   "source": [
    "## Build PPI Network (Yashwi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f171798-5b2a-48ec-855d-f1f195b02a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import scipy.stats as stat\n",
    "import pandas as pd\n",
    "import time, os, random\n",
    "import networkx as nx\n",
    "from networkx.algorithms.link_analysis import pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98f77f0-8ffd-4ae7-a4e6-5c488a1e6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('D:/IUB/Classes/ML for Bioinformatics/Project/utilities/network_utilities_ver4.py').read())\n",
    "exec(open('D:/IUB/Classes/ML for Bioinformatics/Project/utilities/useful_utilities.py').read())\n",
    "start = time.ctime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05fb2153-d798-4bbc-991e-ecd3300ef7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize\n",
    "string_cutoff = 700\n",
    "fo_dir = 'D:/IUB/Classes/ML for Bioinformatics/Project/result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e866829c-9f0c-4b00-92d1-a68e8a08526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data\n",
    "\n",
    "#  Load immunotherapy biomarkers\n",
    "ib = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/Marker_summary.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b737c3fc-a4dc-4408-9299-c3d793742fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing STRING PPI network, Sat May  3 16:37:53 2025\n",
      "network nodes: 16957\n",
      "network edges: 420381\n",
      "\n",
      "\n",
      "Columns in annotation file: Index(['string_protein_id', 'alias', 'source'], dtype='object')\n",
      "make annotation dictionary,  Sat May  3 16:38:11 2025\n",
      "Columns in annotation file: Index(['string_protein_id', 'alias', 'source'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#  Construct Network (STRING network v11)\n",
    "print('constructing STRING PPI network, %s'%time.ctime())\n",
    "tmp_G = nx.Graph()\n",
    "annotation = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/9606.protein.aliases.v11.0.txt.gz', sep=\"\\t\", header=None, comment=\"#\", names=[\"string_protein_id\", \"alias\", \"source\"], usecols=[0, 1, 2]) # STRING network ensembl-geneID mapping\n",
    "net = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/9606.protein.links.v11.0.txt.gz', sep=' ') # STRING network\n",
    "nodes1 = net.values[:,0]\n",
    "nodes2 = net.values[:,1]\n",
    "scores = net.values[:,2]\n",
    "for n1, n2, score in zip(nodes1, nodes2, scores):\n",
    "        if score >= string_cutoff:\n",
    "                tmp_G.add_edge(n1, n2)\n",
    "LCC_genes = max(nx.connected_components(tmp_G), key=len)\n",
    "G = tmp_G.subgraph(LCC_genes) ## Largest Connected Componenets\n",
    "network_nodes = G.nodes()\n",
    "network_edges = G.edges()\n",
    "print('network nodes: %s'%len(network_nodes))\n",
    "print('network edges: %s'%len(network_edges))\n",
    "print('\\n')\n",
    "\n",
    "print(\"Columns in annotation file:\", annotation.columns)\n",
    "#  annotation\n",
    "print('make annotation dictionary, ', time.ctime())\n",
    "anno = pd.DataFrame(data=annotation.loc[annotation['source'].str.contains('HUGO', na=False),:])\n",
    "anno_dic = defaultdict(list) # { ensp : [ genes ] }\n",
    "for ensp, gene in zip(anno['string_protein_id'].tolist(), anno['alias'].tolist()):\n",
    "        anno_dic[ensp].append(gene)\n",
    "\n",
    "print(\"Columns in annotation file:\", annotation.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518e1cac-b14e-4956-9faf-36da4b9b8136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run network propagation, Sat May  3 16:38:40 2025\n",
      "process complete // start: Sat May  3 16:37:42 2025, end: Sat May  3 16:38:40 2025\n"
     ]
    }
   ],
   "source": [
    "## Network propagation\n",
    "print('run network propagation, %s'%time.ctime())\n",
    "\n",
    "for biomarker, feature in zip(ib['Name'].tolist(), ib['Feature'].tolist()):\n",
    "        if '%s.txt'%biomarker in os.listdir(fo_dir):\n",
    "                continue\n",
    "        if not 'target' in feature:\n",
    "                continue\n",
    "        print('\\ttesting %s, %s'%(biomarker, time.ctime()))\n",
    "\n",
    "        output = defaultdict(list)\n",
    "        output_col = ['gene_id', 'string_protein_id', 'propagate_score']\n",
    "\n",
    "        # network propagation\n",
    "        biomarker_genes = ib.loc[ib['Name']==biomarker,:]['Gene_list'].tolist()[0].split(':')\n",
    "        pIDs = annotation.loc[annotation['alias'].isin(biomarker_genes),:]['string_protein_id'].tolist() #only remain biomarker_gene\n",
    "        propagate_input = {}\n",
    "        for node in network_nodes:\n",
    "                propagate_input[node] = 0\n",
    "                if node in pIDs:\n",
    "                        propagate_input[node] = 1\n",
    "        propagate_scores = pagerank(G, personalization=propagate_input, max_iter=100, tol=1e-06) ## NETWORK PROPAGATION\n",
    "\n",
    "        # output\n",
    "        for ensp in list(propagate_scores.keys()):\n",
    "                geneID = 'NA'\n",
    "                if ensp in list(anno_dic.keys()):\n",
    "                        for gene in anno_dic[ensp]:\n",
    "                                geneID = gene\n",
    "                                output['gene_id'].append(geneID)\n",
    "                                output['string_protein_id'].append(ensp)\n",
    "                                output['propagate_score'].append(propagate_scores[ensp])\n",
    "                else:\n",
    "                        geneID = 'NA'\n",
    "                        output['gene_id'].append(geneID)\n",
    "                        output['string_protein_id'].append(ensp)\n",
    "                        output['propagate_score'].append(propagate_scores[ensp])\n",
    "        output = pd.DataFrame(data=output, columns=output_col)\n",
    "        output.to_csv('%s/%s.txt'%(fo_dir, biomarker), sep='\\t', index=False)\n",
    "\n",
    "end = time.ctime()\n",
    "print('process complete // start: %s, end: %s'%(start, end))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f18fece-631b-4090-a29a-87071fa98a9e",
   "metadata": {},
   "source": [
    "## Build Network Graph as input to Models\n",
    "Don't run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4b5a8f9-efc4-430e-9fd7-60e34ddc45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e2a8899-73c4-477f-bd2b-7dee1665d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Filter STRING edges\n",
    "edges_df = (\n",
    "    net[['protein1','protein2','combined_score']]\n",
    "    .query(\"combined_score >= @string_cutoff\")\n",
    "    .loc[:, ['protein1','protein2']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cb0f31a-3a89-4155-a3ed-4f85ec784640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build node2idx mapping\n",
    "nodes = pd.unique(edges_df[['protein1','protein2']].values.ravel())\n",
    "nodes = list(nodes)\n",
    "node2idx = {node: i for i, node in enumerate(nodes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b8797a9-98fd-48f8-80ca-17c1417bbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Build undirected edge_index tensor\n",
    "src = edges_df['protein1'].map(node2idx).tolist()\n",
    "dst = edges_df['protein2'].map(node2idx).tolist()\n",
    "edge_index = torch.tensor([src + dst, dst + src], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2f474-b526-4468-a6cb-bef50c5ac64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the clinical response file\n",
    "clinical = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/Kim_et_al/clinical.txt', sep='\\t')\n",
    "\n",
    "# Extract just the ID → response mapping and save\n",
    "labels_df = clinical[['ID', 'response']].set_index('ID')\n",
    "labels_df.to_csv('response_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db40341-a728-409a-8c7c-c522c7b5a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the clinical response file\n",
    "clinical = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/Kim_et_al/expression_mRNA.norm3.txt', sep='\\t')\n",
    "\n",
    "# Extract just the ID → response mapping and save\n",
    "#labels_df = clinical[['ID', 'response']].set_index('ID')\n",
    "clinical.to_csv('expression_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6698ab7-e6d6-4b63-9abc-fe5da64a971c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Unnamed: 0', 'gene_id', 'PB-16-002', 'PB-16-003', 'PB-16-004', 'PB-16-005', 'PB-16-018', 'PB-16-019', 'PB-16-020', 'PB-16-021']\n",
      "   Unnamed: 0  gene_id  PB-16-002  PB-16-003  PB-16-004  PB-16-005  PB-16-018  \\\n",
      "0           0     A1BG   0.275921  -2.019086  -0.938814  -0.391196   1.027154   \n",
      "1           1     A1CF  -1.124338  -0.391196   0.857254  -0.640667   1.512390   \n",
      "2           2      A2M  -0.857254   0.450744   0.781034  -0.391196  -2.019086   \n",
      "3           3    A2ML1   0.164211  -0.938814  -0.575109  -0.857254   0.000000   \n",
      "4           4  A3GALT2   1.233495  -0.543252   0.511936  -0.543252   0.219723   \n",
      "\n",
      "   PB-16-019  PB-16-020  PB-16-021  ...  PB-16-057  PB-16-059  PB-16-060  \\\n",
      "0   1.359737  -0.781034  -0.164211  ...  -0.450744   1.711675   0.219723   \n",
      "1   0.575109  -1.233495   0.000000  ...  -0.109200   0.109200   0.333005   \n",
      "2  -1.027154  -1.359737  -0.938814  ...   0.709103   0.000000  -0.054519   \n",
      "3   0.709103   0.938814  -1.124338  ...   1.027154  -0.709103   0.391196   \n",
      "4  -0.543252  -0.543252   1.711675  ...   0.781034   0.391196  -0.543252   \n",
      "\n",
      "   PB-16-062  PB-16-063  PB-16-064  PB-16-066  PB-16-067  PB-16-068  PB-16-069  \n",
      "0   0.938814  -0.109200   0.511936   0.450744   0.709103   0.109200   0.857254  \n",
      "1  -0.575109  -1.512390   1.124338  -0.857254  -0.333005   0.275921  -1.711675  \n",
      "2   1.124338  -0.709103   0.897288   2.019086   0.275921   0.109200   1.233495  \n",
      "3  -0.275921  -1.359737   0.640667   1.711675   0.857254   1.512390   0.109200  \n",
      "4  -0.543252  -0.543252   0.640667  -0.543252  -0.543252   0.575109  -0.543252  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Peek at your file’s first few rows & columns\n",
    "expr_raw = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/Kim_et_al/expression_labels.csv', sep=',')  # or sep='\\t' if it’s tab-delim\n",
    "print(\"Columns:\", expr_raw.columns[:10].tolist())\n",
    "print(expr_raw.head())\n",
    "\n",
    "# 2) If you see metadata columns (e.g. Length, Chr, Start, End, Strand), drop them:\n",
    "meta_cols = [c for c in expr_raw.columns \n",
    "             if c.lower() in ('length','chr','start','end','strand')]\n",
    "expr = expr_raw.drop(columns=meta_cols, errors='ignore')\n",
    "\n",
    "# 3) Make the first column your index (gene IDs) if it isn’t already:\n",
    "#    (replace 'Geneid' below with whatever that header actually is)\n",
    "if expr.columns[0].lower() not in ('ensp','gene','geneid'):\n",
    "    expr = expr.set_index(expr.columns[0])\n",
    "else:\n",
    "    expr = expr.set_index(expr.columns[0])\n",
    "\n",
    "# 4) If by chance genes are columns and samples are rows, transpose:\n",
    "#    after the last step genes should be the index and samples the columns.\n",
    "if expr.shape[0] < expr.shape[1]:\n",
    "    expr = expr.T\n",
    "\n",
    "# # 5) Now you have expr: rows=genes, cols=samples.  Save your clean matrix:\n",
    "# expr.to_csv('expression_matrix.csv')\n",
    "# print(\"Cleaned expression matrix shape:\", expr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13ffd7ea-ed04-4b03-837c-2bee67408b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load your expression & labels (make sure paths are correct)\n",
    "expr_df  = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/Kim_et_al/expression_matrix.csv', index_col=0)\n",
    "labels_df = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/Kim_et_al/response_labels.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "651177a1-926e-4ed8-b2d3-22eecf22a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Align expression rows to our node order, fill missing genes with zero\n",
    "expr_df = (\n",
    "    expr_df\n",
    "      .reindex(nodes)\n",
    "      .fillna(0)\n",
    "      .astype(float)        # or .astype('int64'), etc.\n",
    ")\n",
    "expr_df = expr_df.reindex(nodes).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f04098ae-cea3-44dd-ace3-c7df2f5ca358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping non-sample columns from expr_df: {'gene_id'}\n",
      "Built 45 graph examples, each with 17185 nodes.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 1) Find samples present in both expression and labels\n",
    "common_samples = expr_df.columns.intersection(labels_df.index)\n",
    "\n",
    "# 2) Warn about any mismatches\n",
    "extra = set(expr_df.columns) - set(common_samples)\n",
    "if extra:\n",
    "    print(f\"Dropping non-sample columns from expr_df: {extra}\")\n",
    "missing = set(labels_df.index) - set(common_samples)\n",
    "if missing:\n",
    "    print(f\"No expression data for these labels, dropping them: {missing}\")\n",
    "\n",
    "# 3) Reindex expr_df to only those common samples (optional, for cleanliness)\n",
    "expr_df = expr_df[common_samples]\n",
    "\n",
    "# 4) Build Data objects\n",
    "data_list = []\n",
    "for sample in common_samples:\n",
    "    x = torch.tensor(expr_df[sample].values, dtype=torch.float).unsqueeze(1)  # [num_nodes × 1]\n",
    "    y = torch.tensor([labels_df.loc[sample, 'response']], dtype=torch.long)    # [1]\n",
    "    data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "\n",
    "print(f\"Built {len(data_list)} graph examples, each with {expr_df.shape[0]} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0ac7379-91e4-4e3f-8472-cba37fa29c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 45 graphs, each with 17185 nodes.\n"
     ]
    }
   ],
   "source": [
    "# 6) Build a Data object per sample\n",
    "from torch_geometric.data import Data\n",
    "data_list = []\n",
    "for sample in expr_df.columns:\n",
    "    x = torch.tensor(expr_df[sample].values, dtype=torch.float).unsqueeze(1)  # [num_nodes × 1]\n",
    "    y = torch.tensor([labels_df.loc[sample, 'response']], dtype=torch.long)    # [1]\n",
    "    data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
    "\n",
    "print(f\"Built {len(data_list)} graphs, each with {len(nodes)} nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4754f61f-2580-4f8f-975a-c2b237b5f512",
   "metadata": {},
   "source": [
    "## GNN Pipeline for Gastric Cancer Cohort (Kim et al.) and TCGA Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61e96d03-e8db-41c3-940a-2fcdc580572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 841068 edges to string_edges_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "# 1) Load the full STRING PPI table (make sure you’ve downloaded and un-zipped it)\n",
    "net = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/9606.protein.links.v11.0.txt.gz', sep=' ',\n",
    "    usecols=['protein1','protein2','combined_score'])\n",
    "\n",
    "# 2) Filter by your confidence cutoff\n",
    "filtered = (\n",
    "    net[net.combined_score >= string_cutoff]\n",
    "    .loc[:, ['protein1','protein2']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 3) Rename columns to match the rest of your pipeline\n",
    "filtered = filtered.rename(columns={'protein1':'source','protein2':'target'})\n",
    "\n",
    "# 4) Save it\n",
    "filtered.to_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/string_edges_filtered.csv', index=False)\n",
    "print(f\"Saved {len(filtered)} edges to string_edges_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e920bae-a3d6-4feb-a497-cac09b0d7e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. Imports and device setup\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# PyTorch Geometric imports\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "# For cross-validation & hyperparameter tuning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For interpretability\n",
    "from torch_geometric.explain import GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d789b2f-bef9-40a6-8833-87e308860ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. Load PPI network and build edge_index\n",
    "# Assume edges.csv has columns: source, target\n",
    "ppi = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/string_edges_filtered.csv')  # filtered by cutoff\n",
    "# Map ENSP IDs to integer indices\n",
    "genes = sorted(set(ppi.source) | set(ppi.target))\n",
    "gene2idx = {g: i for i, g in enumerate(genes)}\n",
    "# Build edge index\n",
    "edge_index = torch.tensor(\n",
    "    [[gene2idx[s], gene2idx[t]] for s, t in zip(ppi.source, ppi.target)] +\n",
    "    [[gene2idx[t], gene2idx[s]] for s, t in zip(ppi.source, ppi.target)],\n",
    "    dtype=torch.long\n",
    ").t().contiguous().to(torch_device)\n",
    "num_nodes = len(genes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9b77a9-f0df-4af7-8c10-de5b6a40953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_kim = pd.read_csv('D:/IUB/Classes/ML for Bioinformatics/Project/data/Kim_et_al/response_labels.csv', index_col=0)['response']  # Series of 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e82805-a9d0-489b-a8e1-387676a03fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align to PPI node order\n",
    "expr_kim = expr_df.reindex(genes).fillna(0)\n",
    "X_kim = torch.tensor(expr_kim.values.T, dtype=torch.float)\n",
    "y_kim = torch.tensor(labels_kim.loc[expr_kim.columns].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e77ab2e-6987-486c-b002-ddd599fc133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4. Build Data objects for PyG\n",
    "dataset_kim = []\n",
    "for i in range(X_kim.size(0)):\n",
    "    data = Data(x=X_kim[i].unsqueeze(1), edge_index=edge_index, y=y_kim[i].view(1))\n",
    "    dataset_kim.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985eaa8a-956a-4b4c-850c-f43a031f0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 5. Define a simple GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin   = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a611553-363a-47f8-9746-29ab481f78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 6. Training with CV, early stopping, and validation‐loss plotting\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Start overall CV timer\n",
    "cv_start = time.time()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "all_metrics = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_kim, y_kim.numpy()), start=1):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "    fold_start = time.time()\n",
    "    val_losses = []\n",
    "\n",
    "    # 2) Prepare data loaders\n",
    "    train_loader = DataLoader([dataset_kim[i] for i in train_idx], batch_size=8, shuffle=True)\n",
    "    test_loader  = DataLoader([dataset_kim[i] for i in test_idx],  batch_size=8)\n",
    "\n",
    "    # 3) Initialize model & optimizer\n",
    "    model     = GCN(in_channels=1, hidden_channels=64, out_channels=2).to(torch_device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    counter = 0\n",
    "    max_epochs = 100\n",
    "\n",
    "    # 4) Epoch loop\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(torch_device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = F.cross_entropy(out, batch.y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # 5) Compute & record validation loss\n",
    "        val_loss = total_loss / len(train_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        print(f\"Fold {fold}, Epoch {epoch:03d} — val_loss: {val_loss:.4f}, time: {epoch_time:.2f}s\")\n",
    "\n",
    "        # 6) Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'best_model_fold{fold}.pt')\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch} (no improvement for {patience} epochs).\")\n",
    "                break\n",
    "\n",
    "    # 7) Plot the validation‐loss curve for this fold\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, marker='o')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Validation Loss\")\n",
    "    plt.title(f\"Fold {fold} Validation Loss Curve\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 8) Evaluate on the test split\n",
    "    model.load_state_dict(torch.load(f'best_model_fold{fold}.pt'))\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(torch_device)\n",
    "        pred = model(batch).argmax(dim=1)\n",
    "        correct += int((pred == batch.y.view(-1)).sum())\n",
    "        total += batch.num_graphs\n",
    "    acc = correct / total\n",
    "    all_metrics.append(acc)\n",
    "\n",
    "    fold_time = time.time() - fold_start\n",
    "    print(f\"Fold {fold} completed in {fold_time:.2f}s — accuracy: {acc:.4f}\")\n",
    "\n",
    "# 9) Overall CV summary\n",
    "total_cv_time = time.time() - cv_start\n",
    "print(f\"\\n5-fold Accuracies: {all_metrics}\")\n",
    "print(f\"Total CV training time: {total_cv_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494fdb1-b23b-439f-b335-d95d0d1d3931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
